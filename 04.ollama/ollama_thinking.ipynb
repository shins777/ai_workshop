{"cells":[{"cell_type":"markdown","id":"2e33dced-e587-4397-81b3-d6606aa1738a","metadata":{"id":"2e33dced-e587-4397-81b3-d6606aa1738a"},"source":["# Ollama LLM - Thinking model"]},{"cell_type":"markdown","source":["이 예제는 Ollama를 이용해서 local 환경에서 다양한 오픈 소스 모델을 서빙할 수 있는 예제입니다.\n","사용자들은 ollama 사이트에서 특정 오픈소스 모델을 검색해서 사용할 수 있습니다.\n","\n","* Ollama Website\n","  * https://ollama.com/search\n"],"metadata":{"id":"Z64QKFRkPNA7"},"id":"Z64QKFRkPNA7"},{"cell_type":"markdown","source":["## Ollama 다운로드 및 설치"],"metadata":{"id":"bOA4NzY_D_yI"},"id":"bOA4NzY_D_yI"},{"cell_type":"code","execution_count":null,"id":"MqdcFaPLAJfL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38979,"status":"ok","timestamp":1753926737350,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"MqdcFaPLAJfL","outputId":"ed2e052c-d233-4412-8a14-6ec1106cbfdc"},"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Installing ollama to /usr/local\n",">>> Downloading Linux amd64 bundle\n","######################################################################## 100.0%\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n","\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh"]},{"cell_type":"markdown","source":["### Ollama API 서버 실행\n","다른 프로그램(애플리케이션)이 모델을 사용할 수 있도록 백그라운드에서 API 서버를 실행\n","\n","아래 터미널을 클릭해서 창을 열고 아래 명령어를 입력해주세요.\n","```\n","ollama serve\n","```\n","\n","위의 명령어를 실행했을때 로그가 에러없이 올라가면 정상적으로 ollama API 서버가 실행 된것입니다."],"metadata":{"id":"1H0i2wr5DzZZ"},"id":"1H0i2wr5DzZZ"},{"cell_type":"markdown","source":["### Ollama 실행 및 모델 다운로드\n","#### Ollama command 명령어"],"metadata":{"id":"U1jq4p5EGBuM"},"id":"U1jq4p5EGBuM"},{"cell_type":"code","source":["!ollama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyLBXLKjQpgU","executionInfo":{"status":"ok","timestamp":1753928754663,"user_tz":-540,"elapsed":119,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"57db2f0b-f103-450d-eda0-0b73d83627da"},"id":"gyLBXLKjQpgU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Usage:\n","  ollama [flags]\n","  ollama [command]\n","\n","Available Commands:\n","  serve       Start ollama\n","  create      Create a model\n","  show        Show information for a model\n","  run         Run a model\n","  stop        Stop a running model\n","  pull        Pull a model from a registry\n","  push        Push a model to a registry\n","  list        List models\n","  ps          List running models\n","  cp          Copy a model\n","  rm          Remove a model\n","  help        Help about any command\n","\n","Flags:\n","  -h, --help      help for ollama\n","  -v, --version   Show version information\n","\n","Use \"ollama [command] --help\" for more information about a command.\n"]}]},{"cell_type":"markdown","source":["#### 다운로드 받은 모델 리스트 확인"],"metadata":{"id":"Gt-GFy56Tc0a"},"id":"Gt-GFy56Tc0a"},{"cell_type":"code","source":["!ollama list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSkz_K7JLv1F","executionInfo":{"status":"ok","timestamp":1753928104404,"user_tz":-540,"elapsed":109,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"0d778785-61a2-4911-c2f4-f52546fc4de4"},"id":"sSkz_K7JLv1F","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NAME         ID              SIZE      MODIFIED       \n","gemma3:1B    8648f39daa8f    815 MB    21 minutes ago    \n"]}]},{"cell_type":"markdown","source":["#### 모델 다운로드"],"metadata":{"id":"lkYjZNNyTfav"},"id":"lkYjZNNyTfav"},{"cell_type":"code","execution_count":null,"id":"Su2hmPAfBhsc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":615,"status":"ok","timestamp":1753928541626,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"Su2hmPAfBhsc","outputId":"ecb05d8d-b878-4e32-91f3-d2077f27c75f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"]}],"source":["!ollama pull gemma3:4B"]},{"cell_type":"code","execution_count":null,"id":"K37b0ILOCXQS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1753928567620,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"K37b0ILOCXQS","outputId":"4794f054-e639-4658-e04a-ad1fdf5c8465"},"outputs":[{"output_type":"stream","name":"stdout","text":["NAME         ID              SIZE      MODIFIED       \n","gemma3:4B    a2af6cc3eb7f    3.3 GB    26 seconds ago    \n","gemma3:1B    8648f39daa8f    815 MB    28 minutes ago    \n"]}],"source":["!ollama list"]},{"cell_type":"markdown","id":"93b4cbff","metadata":{"id":"93b4cbff"},"source":["## Thinking\n","\n","Ollama의 모델은 \"사고\"를 지원합니다. 사고는 최종 답변을 제공하기 전에 응답에 대해 추론하고 숙고하는 과정입니다.\n","\n","아래에서는 thinking 매개변수와 qwen3:8b 모델을 사용하여 스트리밍 모드와 비스트리밍 모드 모두에서 Ollama 모델에서 사고를 활성화하는 방법을 보여줍니다."]},{"cell_type":"code","source":["!ollama pull qwen3:8b"],"metadata":{"id":"CY2JzqEfVdM4"},"id":"CY2JzqEfVdM4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1e3b1a77","metadata":{"id":"1e3b1a77"},"outputs":[],"source":["from llama_index.llms.ollama import Ollama\n","\n","llm = Ollama(\n","    model=\"qwen3:8b\",\n","    request_timeout=360,\n","    thinking=True,\n","    # Manually set the context window to limit memory usage\n","    context_window=8000,\n",")"]},{"cell_type":"code","execution_count":null,"id":"7d27d440","metadata":{"id":"7d27d440"},"outputs":[],"source":["resp = llm.complete(\"What is 434 / 22?\")"]},{"cell_type":"code","execution_count":null,"id":"c8bab146","metadata":{"id":"c8bab146"},"outputs":[],"source":["print(resp.additional_kwargs[\"thinking\"])"]},{"cell_type":"code","execution_count":null,"id":"d3af8e4e","metadata":{"id":"d3af8e4e"},"outputs":[],"source":["print(resp.text)"]},{"cell_type":"code","execution_count":null,"id":"e4ff2c2a","metadata":{"id":"e4ff2c2a"},"outputs":[],"source":["resp_gen = llm.stream_complete(\"What is 434 / 22?\")\n","\n","thinking_started = False\n","response_started = False\n","\n","for resp in resp_gen:\n","    if resp.additional_kwargs.get(\"thinking_delta\", None):\n","        if not thinking_started:\n","            print(\"\\n\\n-------- Thinking: --------\\n\")\n","            thinking_started = True\n","            response_started = False\n","        print(resp.additional_kwargs[\"thinking_delta\"], end=\"\", flush=True)\n","    if resp.delta:\n","        if not response_started:\n","            print(\"\\n\\n-------- Response: --------\\n\")\n","            response_started = True\n","            thinking_started = False\n","        print(resp.delta, end=\"\", flush=True)"]}],"metadata":{"colab":{"provenance":[{"file_id":"18WuWsK7zy309YEy99_SHG_QZNAooWA2o","timestamp":1753931541808},{"file_id":"13PUcVWxv6ig7nbCoup5SJztmA4SN3YMv","timestamp":1753931431053},{"file_id":"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/ollama.ipynb","timestamp":1753923597294}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}