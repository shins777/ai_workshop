{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2896f7",
   "metadata": {
    "id": "ff2896f7"
   },
   "source": [
    "## Google Multimodal Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5329d6",
   "metadata": {
    "id": "ae5329d6"
   },
   "source": [
    "### Install and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb959f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11966b",
   "metadata": {
    "executionInfo": {
     "elapsed": 16317,
     "status": "ok",
     "timestamp": 1753419971641,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "aa11966b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet google-genai \\\n",
    "#                                 numpy \\\n",
    "#                                 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kwrAYYHFUQ6w",
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753420147334,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "kwrAYYHFUQ6w"
   },
   "outputs": [],
   "source": [
    "#Set environment variables\n",
    "PROJECT_ID = \"ai-hangsik\" \n",
    "REGION = \"us-central1\"\n",
    "# USE_VERTEX_AI = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5492f7d",
   "metadata": {
    "id": "d5492f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=hG6KyIpH9O9z7E9lszSLZfURjdxVrZ&access_type=offline&code_challenge=W72OWdGCjsqqPxidm4qWHRx5vcr3ZP7UvDuSzLdb6-g&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/hangsik/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"ai-hangsik\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "\n",
      "Credentials saved to file: [/Users/hangsik/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"ai-hangsik\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login\n",
    "!gcloud auth application-default set-quota-project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16931ef0",
   "metadata": {
    "id": "16931ef0"
   },
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb97abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.vision_models import Image, MultiModalEmbeddingModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "l3U9eMllUdY2",
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1753420151532,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "l3U9eMllUdY2"
   },
   "outputs": [],
   "source": [
    "# Login to Vertex AI\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "\n",
    "# client = genai.Client(\n",
    "#     vertexai=USE_VERTEX_AI,\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Z51TSX7cDR8R",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1753420153254,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "Z51TSX7cDR8R"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between two embedding arrays\n",
    "def cosine_similarity(embed_1, embed_2):\n",
    "  import numpy as np\n",
    "  from scipy.spatial.distance import cosine\n",
    "\n",
    "  embedding_1 = np.array(embed_1)\n",
    "  embedding_2 = np.array(embed_2)\n",
    "\n",
    "  cosine_similarity = 1 - cosine(embedding_1, embedding_2)\n",
    "  print(f\"Cosine similarity : {cosine_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXAyq_qgArOK",
   "metadata": {
    "id": "cXAyq_qgArOK"
   },
   "source": [
    "### Google Embedding models\n",
    "* https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_pQpd3ho9W9Y",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753420154798,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "_pQpd3ho9W9Y"
   },
   "outputs": [],
   "source": [
    "# Generate embedding using given model, image path, and contextual text\n",
    "\n",
    "def generate_multimodal_embedding(model, image_path, contextual_text):\n",
    "  \n",
    "\tstart_time = time.perf_counter_ns()\n",
    "\n",
    "\tmodel = MultiModalEmbeddingModel.from_pretrained(model)\n",
    "\n",
    "\t# https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.vision_models.MultiModalEmbeddingModel#vertexai_preview_vision_models_MultiModalEmbeddingModel_get_embeddings\n",
    "\t\n",
    "\tembeddings = model.get_embeddings(\n",
    "\n",
    "\t\timage=Image.load_from_file(image_path),\n",
    "\t\tcontextual_text=contextual_text,\n",
    "\t\tdimension=1408, # dimension must be one of 128, 256, 512, 1408\n",
    "\n",
    "\t)\n",
    "\n",
    "\tend_time = time.perf_counter_ns()\n",
    "\n",
    "\tlatency = (end_time - start_time)\n",
    "\tprint(f\"Latency (ns): {latency*1e-6:.2f} ms\")\n",
    "\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c103ac",
   "metadata": {},
   "source": [
    "#### multimodalembedding@001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ig9GMpdd-Czd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1282,
     "status": "ok",
     "timestamp": 1753420157373,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "Ig9GMpdd-Czd",
    "outputId": "0f10b600-83dd-4b3a-feee-7fbcb2a9a12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency (ns): 4222.95 ms\n",
      "Generated embedding vector for image (length: 1408):\n",
      "Generated embedding vector for text  (length: 1408):\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"multimodalembedding@001\"\n",
    "IMAGE_PATH = \"gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\"\n",
    "TEXT = \"Colosseum\"\n",
    "\n",
    "\n",
    "embeddings = generate_multimodal_embedding(model = MODEL,\n",
    "                                image_path = IMAGE_PATH,\n",
    "                                contextual_text = TEXT) \n",
    "\n",
    "print(f\"Generated embedding vector for image (length: {len(embeddings.image_embedding)}):\")\n",
    "print(f\"Generated embedding vector for text  (length: {len(embeddings.text_embedding)}):\")\n",
    "\n",
    "# cosine_similarity(embed_1, embed_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0810cc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "529a9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "    \n",
    "def generate_text_embedding(model, contextual_text):\n",
    "\n",
    "    \n",
    "\tstart_time = time.perf_counter_ns() \n",
    "\n",
    "\tmodel = TextEmbeddingModel.from_pretrained(model)\n",
    "\n",
    "\t# https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#parameter-list\n",
    "\n",
    "\tinstances = [\n",
    "\t\t{\n",
    "\t\t\t\"task_type\": \"RETRIEVAL_QUERY\",\n",
    "\t\t\t\"content\": contextual_text,\n",
    "\t\t\t#\"title\": \"content's title if needed\"\n",
    "\t\t}\n",
    "\t]\n",
    "\n",
    "\tparameters = {\n",
    "\t\t\"autoTruncate\" : True,\n",
    "\t\t\"outputDimensionality\": 1024,  # dimension must be one of 128, 256, 512, 1024\n",
    "\n",
    "\t}\n",
    "\n",
    "\tembeddings = model.get_embeddings(\n",
    "\n",
    "\t\tinstances=instances,\n",
    "\t\tparameters=parameters\n",
    "\t\t\n",
    "\n",
    "\t)\n",
    "\n",
    "\tend_time = time.perf_counter_ns()\n",
    "\n",
    "\tlatency = (end_time - start_time)\n",
    "\tprint(f\"Latency (ns): {latency*1e-6:.2f} ms\")\n",
    "\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e70a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangsik/Documents/GitHub/ai_workshop/.venv/lib/python3.12/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_TextEmbeddingModel.get_embeddings() got an unexpected keyword argument 'instances'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m MODEL = \u001b[33m\"\u001b[39m\u001b[33mtext-multilingual-embedding-002\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m TEXT = \u001b[33m\"\u001b[39m\u001b[33m고양이가 자전거를 타고 간다\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m embeddings = \u001b[43mgenerate_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mcontextual_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated embedding vector for text (length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings.text_embedding)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_text_embedding\u001b[39m\u001b[34m(model, contextual_text)\u001b[39m\n\u001b[32m     12\u001b[39m instances = [\n\u001b[32m     13\u001b[39m \t{\n\u001b[32m     14\u001b[39m \t\t\u001b[33m\"\u001b[39m\u001b[33mtask_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \t}\n\u001b[32m     18\u001b[39m ]\n\u001b[32m     20\u001b[39m parameters = {\n\u001b[32m     21\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33mautoTruncate\u001b[39m\u001b[33m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     22\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33moutputDimensionality\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1024\u001b[39m,  \u001b[38;5;66;03m# dimension must be one of 128, 256, 512, 1024\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[43m\t\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m end_time = time.perf_counter_ns()\n\u001b[32m     36\u001b[39m latency = (end_time - start_time)\n",
      "\u001b[31mTypeError\u001b[39m: _TextEmbeddingModel.get_embeddings() got an unexpected keyword argument 'instances'"
     ]
    }
   ],
   "source": [
    "MODEL = \"text-multilingual-embedding-002\"\n",
    "TEXT = \"고양이가 자전거를 타고 간다\"\n",
    "\n",
    "\n",
    "embeddings = generate_text_embedding(model = MODEL,\n",
    "                                contextual_text = TEXT) \n",
    "\n",
    "print(f\"Generated embedding vector for text (length: {len(embeddings.text_embedding)}):\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5xH-7ZYBzFgA",
   "metadata": {
    "id": "5xH-7ZYBzFgA"
   },
   "source": [
    "## End of Document"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "11R51JG9-2NCZhHRTqNzJPXqfFyV95uoI",
     "timestamp": 1753335275054
    },
    {
     "file_id": "10gZJu0E2iUQmlLPw26rXPjCCR4uFDwZ6",
     "timestamp": 1753325541712
    },
    {
     "file_id": "1aEpqQqoFTSDfbaZIxp_vmMKKVh0SupO5",
     "timestamp": 1753315305740
    },
    {
     "file_id": "1GoMGdpjOWcSfnDeO0E7Alge4UD1ThQu6",
     "timestamp": 1753312386672
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
